# StrengthWise è³‡æ–™åº«é·ç§»å¯¦ä½œæŒ‡å—

**ç‹€æ…‹**: ğŸš€ åŸ·è¡Œä¸­  
**ç›®æ¨™**: å¾ Firebase Firestore é·ç§»åˆ° Supabase PostgreSQL  
**é–‹å§‹æ—¥æœŸ**: 2025-12-25

---

## ğŸ¯ åŸ·è¡Œæ‘˜è¦

åŸºæ–¼ `docs/database_migration_analysis.md` çš„è©•ä¼°çµæœï¼Œæˆ‘å€‘æ±ºå®šæ¡ç”¨ **å®Œå…¨é·ç§»åˆ° Supabase PostgreSQL** çš„æ–¹æ¡ˆã€‚æœ¬æ–‡æª”è¨˜éŒ„å®Œæ•´çš„å¯¦ä½œæ­¥é©Ÿã€æŠ€è¡“ç´°ç¯€å’Œæ³¨æ„äº‹é …ã€‚

### æ±ºç­–ç†ç”±

1. **æˆæœ¬å¯é æ¸¬**: Supabase Pro $25/æœˆï¼Œæ”¯æ´åˆ° 10,000 æ´»èºç”¨æˆ¶
2. **å®Œæ•´ SQL æ”¯æ´**: è¤‡é›œæŸ¥è©¢ã€JOINã€èšåˆå‡½æ•¸
3. **æ•ˆèƒ½æ›´å¥½**: ç´¢å¼•å„ªåŒ–ã€æŸ¥è©¢è¨ˆåŠƒ
4. **é›¢ç·šå„ªå…ˆ**: é…åˆ PowerSync å¯¦ç¾æœ¬åœ° SQLite åŒæ­¥

---

## ğŸ“‹ é·ç§»è·¯ç·šåœ–

### éšæ®µä¸€ï¼šåœ°åŸºå·¥ç¨‹ï¼ˆWeek 1-2ï¼‰â³ é€²è¡Œä¸­

#### 1.1 Supabase å°ˆæ¡ˆè¨­ç½®
- [x] å‰µå»º Supabase å°ˆæ¡ˆ
- [x] å–å¾— API Keys å’Œ é€£æ¥è³‡è¨Š
- [ ] è¨­ç½®ç’°å¢ƒè®Šæ•¸ç®¡ç†
- [ ] é…ç½® Row Level Security (RLS)

#### 1.2 è³‡æ–™åº« Schema è¨­è¨ˆ
- [ ] è¨­è¨ˆæ­£è¦åŒ–çš„ PostgreSQL Schema
- [ ] å»ºç«‹ Migration è…³æœ¬
- [ ] å®šç¾©å¤–éµé—œè¯
- [ ] å»ºç«‹ç´¢å¼•ç­–ç•¥

#### 1.3 è³‡æ–™é·ç§»
- [ ] æ’°å¯« Python é·ç§»è…³æœ¬
- [ ] åŸ·è¡Œè³‡æ–™è½‰æ›ï¼ˆNoSQL â†’ SQLï¼‰
- [ ] é©—è­‰è³‡æ–™å®Œæ•´æ€§
- [ ] å‚™ä»½é©—è­‰

### éšæ®µäºŒï¼šFlutter æ•´åˆï¼ˆWeek 3-4ï¼‰
- [ ] å®‰è£ supabase-flutter SDK
- [ ] é‡æ§‹ Service å±¤ï¼ˆä½¿ç”¨ Supabase Clientï¼‰
- [ ] å¯¦ä½œé›¢ç·šå„ªå…ˆæ¶æ§‹ï¼ˆSQLite + PowerSyncï¼‰
- [ ] æ¸¬è©¦èˆ‡é©—è­‰

### éšæ®µä¸‰ï¼šéƒ¨ç½²èˆ‡é©—è­‰ï¼ˆWeek 5ï¼‰
- [ ] ç°åº¦ç™¼å¸ƒï¼ˆéƒ¨åˆ†ç”¨æˆ¶æ¸¬è©¦ï¼‰
- [ ] æ•ˆèƒ½ç›£æ§
- [ ] éŒ¯èª¤è™•ç†å®Œå–„
- [ ] æ­£å¼ä¸Šç·š

---

## ğŸ—„ï¸ PostgreSQL Schema è¨­è¨ˆ

### æ ¸å¿ƒåŸå‰‡

1. **æ­£è¦åŒ–**: é¿å…è³‡æ–™é‡è¤‡ï¼Œä½¿ç”¨å¤–éµé—œè¯
2. **å¯æ“´å±•æ€§**: é ç•™æœªä¾†åŠŸèƒ½çš„æ¬„ä½ç©ºé–“
3. **æ•ˆèƒ½**: åˆç†çš„ç´¢å¼•ç­–ç•¥
4. **å®‰å…¨æ€§**: Row Level Security (RLS) ç¢ºä¿è³‡æ–™éš”é›¢

### è¡¨æ ¼è¨­è¨ˆ

#### 1. users è¡¨
```sql
CREATE TABLE users (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  email TEXT UNIQUE NOT NULL,
  display_name TEXT,
  avatar_url TEXT,
  age INTEGER,
  gender TEXT,
  height DECIMAL(5,2),
  weight DECIMAL(5,2),
  unit_system TEXT DEFAULT 'metric',
  is_coach BOOLEAN DEFAULT false,
  is_student BOOLEAN DEFAULT true,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- RLS ç­–ç•¥ï¼šç”¨æˆ¶åªèƒ½æŸ¥çœ‹è‡ªå·±çš„è³‡æ–™
ALTER TABLE users ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view own data"
  ON users FOR SELECT
  USING (auth.uid() = id);

CREATE POLICY "Users can update own data"
  ON users FOR UPDATE
  USING (auth.uid() = id);
```

#### 2. exercises è¡¨ï¼ˆå‹•ä½œåº«ï¼‰
```sql
CREATE TABLE exercises (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  name TEXT NOT NULL,
  name_en TEXT,
  action_name TEXT,
  training_type TEXT,
  body_part TEXT,
  body_parts TEXT[], -- Array for multiple body parts
  specific_muscle TEXT,
  equipment TEXT,
  equipment_category TEXT,
  equipment_subcategory TEXT,
  joint_type TEXT,
  level1 TEXT,
  level2 TEXT,
  level3 TEXT,
  level4 TEXT,
  level5 TEXT,
  description TEXT,
  image_url TEXT,
  video_url TEXT,
  user_id UUID REFERENCES users(id), -- NULL = ç³»çµ±å…§å»ºï¼Œæœ‰å€¼ = ç”¨æˆ¶è‡ªå®šç¾©
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- ç´¢å¼•ï¼šå¸¸ç”¨æŸ¥è©¢æ¬„ä½
CREATE INDEX idx_exercises_training_type ON exercises(training_type);
CREATE INDEX idx_exercises_body_part ON exercises(body_part);
CREATE INDEX idx_exercises_user_id ON exercises(user_id);

-- RLSï¼šç³»çµ±å‹•ä½œæ‰€æœ‰äººå¯è¦‹ï¼Œè‡ªå®šç¾©å‹•ä½œåªæœ‰å‰µå»ºè€…å¯è¦‹
ALTER TABLE exercises ENABLE ROW LEVEL SECURITY;

CREATE POLICY "System exercises are viewable by everyone"
  ON exercises FOR SELECT
  USING (user_id IS NULL OR user_id = auth.uid());
```

#### 3. workout_plans è¡¨ï¼ˆè¨“ç·´è¨ˆåŠƒï¼‰
```sql
CREATE TABLE workout_plans (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  trainee_id UUID NOT NULL REFERENCES users(id),
  creator_id UUID NOT NULL REFERENCES users(id),
  title TEXT NOT NULL,
  description TEXT,
  plan_type TEXT DEFAULT 'self',
  ui_plan_type TEXT,
  scheduled_date TIMESTAMPTZ NOT NULL,
  completed BOOLEAN DEFAULT false,
  completed_date TIMESTAMPTZ,
  training_time TIMESTAMPTZ,
  total_exercises INTEGER DEFAULT 0,
  total_sets INTEGER DEFAULT 0,
  total_volume DECIMAL(10,2) DEFAULT 0,
  note TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- ç´¢å¼•ï¼šé«˜é »æŸ¥è©¢æ¬„ä½
CREATE INDEX idx_workout_plans_user_id ON workout_plans(user_id);
CREATE INDEX idx_workout_plans_trainee_id ON workout_plans(trainee_id);
CREATE INDEX idx_workout_plans_scheduled_date ON workout_plans(scheduled_date);
CREATE INDEX idx_workout_plans_completed ON workout_plans(completed);

-- RLSï¼šç”¨æˆ¶å¯ä»¥æŸ¥çœ‹è‡ªå·±ä½œç‚º trainee æˆ– creator çš„è¨ˆåŠƒ
ALTER TABLE workout_plans ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view own workout plans"
  ON workout_plans FOR SELECT
  USING (trainee_id = auth.uid() OR creator_id = auth.uid());
```

#### 4. workout_exercises è¡¨ï¼ˆè¨“ç·´è¨ˆåŠƒä¸­çš„å‹•ä½œï¼‰
```sql
CREATE TABLE workout_exercises (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  workout_plan_id UUID NOT NULL REFERENCES workout_plans(id) ON DELETE CASCADE,
  exercise_id UUID NOT NULL REFERENCES exercises(id),
  exercise_name TEXT NOT NULL, -- å†—é¤˜æ¬„ä½ï¼Œé¿å… JOIN
  order_index INTEGER NOT NULL,
  completed BOOLEAN DEFAULT false,
  rest_time INTEGER DEFAULT 90, -- ç§’
  notes TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ç´¢å¼•
CREATE INDEX idx_workout_exercises_plan_id ON workout_exercises(workout_plan_id);
CREATE INDEX idx_workout_exercises_order ON workout_exercises(workout_plan_id, order_index);

-- RLSï¼šç¹¼æ‰¿ workout_plans çš„æ¬Šé™
ALTER TABLE workout_exercises ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view exercises in own plans"
  ON workout_exercises FOR SELECT
  USING (
    EXISTS (
      SELECT 1 FROM workout_plans
      WHERE workout_plans.id = workout_exercises.workout_plan_id
      AND (workout_plans.trainee_id = auth.uid() OR workout_plans.creator_id = auth.uid())
    )
  );
```

#### 5. workout_sets è¡¨ï¼ˆçµ„æ•¸è¨˜éŒ„ï¼‰
```sql
CREATE TABLE workout_sets (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  workout_exercise_id UUID NOT NULL REFERENCES workout_exercises(id) ON DELETE CASCADE,
  set_number INTEGER NOT NULL,
  reps INTEGER,
  weight DECIMAL(6,2),
  completed BOOLEAN DEFAULT false,
  timestamp TIMESTAMPTZ,
  note TEXT,
  rpe INTEGER, -- Rate of Perceived Exertion (1-10)
  set_type TEXT DEFAULT 'working', -- 'warmup', 'working', 'drop_set', 'failure'
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ç´¢å¼•
CREATE INDEX idx_workout_sets_exercise_id ON workout_sets(workout_exercise_id);

-- RLSï¼šç¹¼æ‰¿æ¬Šé™
ALTER TABLE workout_sets ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view sets in own exercises"
  ON workout_sets FOR SELECT
  USING (
    EXISTS (
      SELECT 1 FROM workout_exercises we
      JOIN workout_plans wp ON we.workout_plan_id = wp.id
      WHERE we.id = workout_sets.workout_exercise_id
      AND (wp.trainee_id = auth.uid() OR wp.creator_id = auth.uid())
    )
  );
```

#### 6. body_parts è¡¨
```sql
CREATE TABLE body_parts (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  name TEXT UNIQUE NOT NULL,
  description TEXT,
  count INTEGER DEFAULT 0,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- å…¬é–‹å¯è®€
ALTER TABLE body_parts ENABLE ROW LEVEL SECURITY;
CREATE POLICY "Body parts are viewable by everyone"
  ON body_parts FOR SELECT TO authenticated
  USING (true);
```

#### 7. exercise_types è¡¨
```sql
CREATE TABLE exercise_types (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  name TEXT UNIQUE NOT NULL,
  description TEXT,
  count INTEGER DEFAULT 0,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- å…¬é–‹å¯è®€
ALTER TABLE exercise_types ENABLE ROW LEVEL SECURITY;
CREATE POLICY "Exercise types are viewable by everyone"
  ON exercise_types FOR SELECT TO authenticated
  USING (true);
```

#### 8. notes è¡¨
```sql
CREATE TABLE notes (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
  title TEXT,
  text_content TEXT,
  drawing_points JSONB, -- å„²å­˜ç¹ªåœ–è³‡æ–™
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- ç´¢å¼•
CREATE INDEX idx_notes_user_id ON notes(user_id);

-- RLS
ALTER TABLE notes ENABLE ROW LEVEL SECURITY;
CREATE POLICY "Users can manage own notes"
  ON notes FOR ALL
  USING (user_id = auth.uid());
```

---

## ğŸ”§ è³‡æ–™é·ç§»è…³æœ¬

### ç’°å¢ƒè¨­ç½®

#### 1. å®‰è£ä¾è³´
```bash
pip install supabase python-dotenv ijson
```

#### 2. ç’°å¢ƒè®Šæ•¸é…ç½®

å‰µå»º `.env` æª”æ¡ˆï¼ˆâš ï¸ ä¸è¦æäº¤åˆ° gitï¼‰ï¼š
```bash
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key
```

å‰µå»º `.env.example` æ¨¡æ¿ï¼š
```bash
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key-here
```

### é·ç§»è…³æœ¬æ¶æ§‹

å‰µå»º `scripts/migrate_to_supabase.py`ï¼š

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
StrengthWise è³‡æ–™åº«é·ç§»è…³æœ¬
å¾ Firebase Firestore (JSON) é·ç§»åˆ° Supabase PostgreSQL
"""

import json
import os
import sys
from typing import Dict, List, Any
from datetime import datetime
from supabase import create_client, Client
from dotenv import load_dotenv
import ijson

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# åˆå§‹åŒ– Supabase Client
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ éŒ¯èª¤ï¼šè«‹è¨­ç½® SUPABASE_URL å’Œ SUPABASE_SERVICE_ROLE_KEY ç’°å¢ƒè®Šæ•¸")
    sys.exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

class DataMigrator:
    """è³‡æ–™é·ç§»ä¸»é¡åˆ¥"""
    
    def __init__(self, json_file_path: str):
        self.json_file_path = json_file_path
        self.stats = {
            'users': 0,
            'exercises': 0,
            'workout_plans': 0,
            'workout_exercises': 0,
            'workout_sets': 0,
            'body_parts': 0,
            'exercise_types': 0,
            'notes': 0,
            'errors': []
        }
    
    def run(self):
        """åŸ·è¡Œå®Œæ•´é·ç§»æµç¨‹"""
        print("=" * 60)
        print("ğŸš€ StrengthWise è³‡æ–™åº«é·ç§»")
        print("=" * 60)
        
        # è¼‰å…¥ JSON è³‡æ–™
        print("\nğŸ“‚ è¼‰å…¥è³‡æ–™æª”æ¡ˆ...")
        with open(self.json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        collections = data.get('collections', {})
        
        # ä¾åºé·ç§»ï¼ˆæ³¨æ„é †åºï¼šå…ˆçˆ¶è¡¨ï¼Œå¾Œå­è¡¨ï¼‰
        self.migrate_body_parts(collections.get('bodyParts', {}))
        self.migrate_exercise_types(collections.get('exerciseTypes', {}))
        self.migrate_users(collections.get('users', {}))
        self.migrate_exercises(collections.get('exercise', {}))
        self.migrate_workout_plans(collections.get('workoutPlans', {}))
        self.migrate_notes(collections.get('notes', {}))
        
        # è¼¸å‡ºçµ±è¨ˆ
        self.print_stats()
    
    def migrate_users(self, users_data: Dict):
        """é·ç§»ç”¨æˆ¶è³‡æ–™"""
        print("\nğŸ‘¤ é·ç§»ç”¨æˆ¶è³‡æ–™...")
        
        if not users_data or 'sample_documents' not in users_data:
            print("  âš ï¸  ç„¡ç”¨æˆ¶è³‡æ–™")
            return
        
        users_batch = []
        for doc in users_data['sample_documents']:
            user = doc['data']
            users_batch.append({
                'id': doc['id'],
                'email': user.get('email'),
                'display_name': user.get('displayName'),
                'avatar_url': user.get('photoURL'),
                'age': user.get('age'),
                'gender': user.get('gender'),
                'height': user.get('height'),
                'weight': user.get('weight'),
                'unit_system': user.get('unitSystem', 'metric'),
                'is_coach': user.get('isCoach', False),
                'is_student': user.get('isStudent', True),
                'created_at': self.parse_timestamp(user.get('profileCreatedAt')),
                'updated_at': self.parse_timestamp(user.get('profileUpdatedAt'))
            })
        
        if users_batch:
            try:
                response = supabase.table("users").upsert(users_batch).execute()
                self.stats['users'] = len(users_batch)
                print(f"  âœ… æˆåŠŸé·ç§» {len(users_batch)} å€‹ç”¨æˆ¶")
            except Exception as e:
                error_msg = f"é·ç§»ç”¨æˆ¶å¤±æ•—: {e}"
                print(f"  âŒ {error_msg}")
                self.stats['errors'].append(error_msg)
    
    def migrate_exercises(self, exercises_data: Dict):
        """é·ç§»å‹•ä½œåº«"""
        print("\nğŸ’ª é·ç§»å‹•ä½œåº«...")
        
        if not exercises_data or 'sample_documents' not in exercises_data:
            print("  âš ï¸  ç„¡å‹•ä½œè³‡æ–™")
            return
        
        exercises_batch = []
        for doc in exercises_data['sample_documents']:
            exercise = doc['data']
            exercises_batch.append({
                'id': doc['id'],
                'name': exercise.get('name'),
                'name_en': exercise.get('nameEn'),
                'action_name': exercise.get('actionName'),
                'training_type': exercise.get('trainingType'),
                'body_part': exercise.get('bodyPart'),
                'body_parts': exercise.get('bodyParts', []),
                'specific_muscle': exercise.get('specificMuscle'),
                'equipment': exercise.get('equipment'),
                'equipment_category': exercise.get('equipmentCategory'),
                'equipment_subcategory': exercise.get('equipmentSubcategory'),
                'joint_type': exercise.get('jointType'),
                'level1': exercise.get('level1'),
                'level2': exercise.get('level2'),
                'level3': exercise.get('level3'),
                'level4': exercise.get('level4'),
                'level5': exercise.get('level5'),
                'description': exercise.get('description'),
                'image_url': exercise.get('imageUrl'),
                'video_url': exercise.get('videoUrl'),
                'user_id': None,  # ç³»çµ±å…§å»ºå‹•ä½œ
                'created_at': self.parse_timestamp(exercise.get('createdAt'))
            })
        
        # æ‰¹æ¬¡å¯«å…¥ï¼ˆæ¯æ¬¡ 100 ç­†ï¼‰
        batch_size = 100
        for i in range(0, len(exercises_batch), batch_size):
            batch = exercises_batch[i:i+batch_size]
            try:
                supabase.table("exercises").upsert(batch).execute()
                self.stats['exercises'] += len(batch)
                print(f"  âœ… é·ç§»é€²åº¦: {self.stats['exercises']}/{len(exercises_batch)}")
            except Exception as e:
                error_msg = f"é·ç§»å‹•ä½œå¤±æ•— (batch {i}): {e}"
                print(f"  âŒ {error_msg}")
                self.stats['errors'].append(error_msg)
    
    def migrate_workout_plans(self, plans_data: Dict):
        """é·ç§»è¨“ç·´è¨ˆåŠƒï¼ˆåŒ…å« exercises å’Œ setsï¼‰"""
        print("\nğŸ“‹ é·ç§»è¨“ç·´è¨ˆåŠƒ...")
        
        if not plans_data or 'sample_documents' not in plans_data:
            print("  âš ï¸  ç„¡è¨“ç·´è¨ˆåŠƒè³‡æ–™")
            return
        
        for doc in plans_data['sample_documents']:
            plan = doc['data']
            
            # 1. æ’å…¥ workout_plan
            plan_record = {
                'id': doc['id'],
                'user_id': plan.get('userId'),
                'trainee_id': plan.get('traineeId'),
                'creator_id': plan.get('creatorId'),
                'title': plan.get('title'),
                'description': plan.get('description'),
                'plan_type': plan.get('planType'),
                'ui_plan_type': plan.get('uiPlanType'),
                'scheduled_date': self.parse_timestamp(plan.get('scheduledDate')),
                'completed': plan.get('completed', False),
                'completed_date': self.parse_timestamp(plan.get('completedDate')),
                'training_time': self.parse_timestamp(plan.get('trainingTime')),
                'total_exercises': plan.get('totalExercises', 0),
                'total_sets': plan.get('totalSets', 0),
                'total_volume': plan.get('totalVolume', 0),
                'note': plan.get('note'),
                'created_at': self.parse_timestamp(plan.get('createdAt')),
                'updated_at': self.parse_timestamp(plan.get('updatedAt'))
            }
            
            try:
                supabase.table("workout_plans").upsert([plan_record]).execute()
                self.stats['workout_plans'] += 1
                
                # 2. æ’å…¥ workout_exercises å’Œ workout_sets
                exercises = plan.get('exercises', [])
                for idx, exercise in enumerate(exercises):
                    exercise_record = {
                        'workout_plan_id': doc['id'],
                        'exercise_id': exercise.get('exerciseId'),
                        'exercise_name': exercise.get('exerciseName'),
                        'order_index': idx,
                        'completed': exercise.get('completed', False),
                        'rest_time': exercise.get('restTime', 90),
                        'notes': exercise.get('notes')
                    }
                    
                    ex_response = supabase.table("workout_exercises").insert([exercise_record]).execute()
                    workout_exercise_id = ex_response.data[0]['id']
                    self.stats['workout_exercises'] += 1
                    
                    # 3. æ’å…¥ sets
                    sets = exercise.get('sets', [])
                    if isinstance(sets, list):
                        sets_batch = []
                        for set_data in sets:
                            if isinstance(set_data, dict):
                                sets_batch.append({
                                    'workout_exercise_id': workout_exercise_id,
                                    'set_number': set_data.get('setNumber'),
                                    'reps': set_data.get('reps'),
                                    'weight': set_data.get('weight'),
                                    'completed': set_data.get('completed', False),
                                    'timestamp': set_data.get('timestamp'),
                                    'note': set_data.get('note')
                                })
                        
                        if sets_batch:
                            supabase.table("workout_sets").insert(sets_batch).execute()
                            self.stats['workout_sets'] += len(sets_batch)
                
                print(f"  âœ… è¨ˆåŠƒ '{plan.get('title')}' ({len(exercises)} å‹•ä½œ)")
                
            except Exception as e:
                error_msg = f"é·ç§»è¨ˆåŠƒå¤±æ•— ({doc['id']}): {e}"
                print(f"  âŒ {error_msg}")
                self.stats['errors'].append(error_msg)
    
    def migrate_body_parts(self, body_parts_data: Dict):
        """é·ç§»èº«é«”éƒ¨ä½"""
        print("\nğŸ¦´ é·ç§»èº«é«”éƒ¨ä½...")
        
        if not body_parts_data or 'sample_documents' not in body_parts_data:
            print("  âš ï¸  ç„¡èº«é«”éƒ¨ä½è³‡æ–™")
            return
        
        batch = []
        for doc in body_parts_data['sample_documents']:
            part = doc['data']
            batch.append({
                'id': doc['id'],
                'name': part.get('name'),
                'description': part.get('description'),
                'count': part.get('count', 0)
            })
        
        if batch:
            try:
                supabase.table("body_parts").upsert(batch).execute()
                self.stats['body_parts'] = len(batch)
                print(f"  âœ… æˆåŠŸé·ç§» {len(batch)} å€‹èº«é«”éƒ¨ä½")
            except Exception as e:
                error_msg = f"é·ç§»èº«é«”éƒ¨ä½å¤±æ•—: {e}"
                print(f"  âŒ {error_msg}")
                self.stats['errors'].append(error_msg)
    
    def migrate_exercise_types(self, types_data: Dict):
        """é·ç§»å‹•ä½œé¡å‹"""
        print("\nğŸ‹ï¸ é·ç§»å‹•ä½œé¡å‹...")
        
        if not types_data or 'sample_documents' not in types_data:
            print("  âš ï¸  ç„¡å‹•ä½œé¡å‹è³‡æ–™")
            return
        
        batch = []
        for doc in types_data['sample_documents']:
            type_data = doc['data']
            batch.append({
                'id': doc['id'],
                'name': type_data.get('name'),
                'description': type_data.get('description'),
                'count': type_data.get('count', 0)
            })
        
        if batch:
            try:
                supabase.table("exercise_types").upsert(batch).execute()
                self.stats['exercise_types'] = len(batch)
                print(f"  âœ… æˆåŠŸé·ç§» {len(batch)} å€‹å‹•ä½œé¡å‹")
            except Exception as e:
                error_msg = f"é·ç§»å‹•ä½œé¡å‹å¤±æ•—: {e}"
                print(f"  âŒ {error_msg}")
                self.stats['errors'].append(error_msg)
    
    def migrate_notes(self, notes_data: Dict):
        """é·ç§»ç­†è¨˜"""
        print("\nğŸ“ é·ç§»ç­†è¨˜...")
        
        if not notes_data or 'sample_documents' not in notes_data:
            print("  âš ï¸  ç„¡ç­†è¨˜è³‡æ–™")
            return
        
        batch = []
        for doc in notes_data['sample_documents']:
            note = doc['data']
            batch.append({
                'id': doc['id'],
                'user_id': note.get('userId'),
                'title': note.get('title'),
                'text_content': note.get('textContent'),
                'drawing_points': note.get('drawingPoints'),
                'created_at': self.parse_timestamp(note.get('createdAt')),
                'updated_at': self.parse_timestamp(note.get('updatedAt'))
            })
        
        if batch:
            try:
                supabase.table("notes").upsert(batch).execute()
                self.stats['notes'] = len(batch)
                print(f"  âœ… æˆåŠŸé·ç§» {len(batch)} ç­†ç­†è¨˜")
            except Exception as e:
                error_msg = f"é·ç§»ç­†è¨˜å¤±æ•—: {e}"
                print(f"  âŒ {error_msg}")
                self.stats['errors'].append(error_msg)
    
    def parse_timestamp(self, ts):
        """è§£æ Firestore timestamp"""
        if not ts:
            return None
        if isinstance(ts, str):
            # å·²ç¶“æ˜¯ ISO æ ¼å¼
            return ts
        if isinstance(ts, int):
            # Unix timestamp (ms)
            return datetime.fromtimestamp(ts / 1000).isoformat()
        return None
    
    def print_stats(self):
        """è¼¸å‡ºé·ç§»çµ±è¨ˆ"""
        print("\n" + "=" * 60)
        print("ğŸ“Š é·ç§»çµ±è¨ˆ")
        print("=" * 60)
        print(f"ç”¨æˆ¶:         {self.stats['users']}")
        print(f"å‹•ä½œåº«:       {self.stats['exercises']}")
        print(f"è¨“ç·´è¨ˆåŠƒ:     {self.stats['workout_plans']}")
        print(f"è¨“ç·´å‹•ä½œ:     {self.stats['workout_exercises']}")
        print(f"çµ„æ•¸è¨˜éŒ„:     {self.stats['workout_sets']}")
        print(f"èº«é«”éƒ¨ä½:     {self.stats['body_parts']}")
        print(f"å‹•ä½œé¡å‹:     {self.stats['exercise_types']}")
        print(f"ç­†è¨˜:         {self.stats['notes']}")
        print(f"éŒ¯èª¤æ•¸:       {len(self.stats['errors'])}")
        
        if self.stats['errors']:
            print("\nâŒ éŒ¯èª¤æ¸…å–®:")
            for error in self.stats['errors']:
                print(f"  - {error}")
        
        print("=" * 60)

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹å¼: python migrate_to_supabase.py <json_file_path>")
        sys.exit(1)
    
    json_file = sys.argv[1]
    
    if not os.path.exists(json_file):
        print(f"âŒ éŒ¯èª¤ï¼šæª”æ¡ˆä¸å­˜åœ¨ {json_file}")
        sys.exit(1)
    
    migrator = DataMigrator(json_file)
    migrator.run()
    
    print("\nâœ… é·ç§»å®Œæˆï¼")

if __name__ == "__main__":
    main()
```

---

## ğŸ” å®‰å…¨æ€§é…ç½®

### 1. ç’°å¢ƒè®Šæ•¸ç®¡ç†

**âš ï¸ é‡è¦**ï¼šSecret Key çµ•å°ä¸èƒ½æäº¤åˆ° gitï¼

1. å°‡ `.env` åŠ å…¥ `.gitignore`
2. åªæäº¤ `.env.example` ä½œç‚ºæ¨¡æ¿
3. åœ¨éƒ¨ç½²ç’°å¢ƒä¸­è¨­ç½®ç’°å¢ƒè®Šæ•¸

### 2. Row Level Security (RLS)

å·²åœ¨ä¸Šè¿° Schema ä¸­é…ç½®ï¼Œç¢ºä¿ï¼š
- ç”¨æˆ¶åªèƒ½å­˜å–è‡ªå·±çš„è³‡æ–™
- ç³»çµ±è³‡æ–™ï¼ˆå‹•ä½œåº«ã€èº«é«”éƒ¨ä½ï¼‰æ‰€æœ‰äººå¯è®€
- æ•™ç·´å¯ä»¥æŸ¥çœ‹å­¸å“¡çš„è¨“ç·´è¨ˆåŠƒ

---

## ğŸ“± Flutter æ•´åˆ

### 1. å®‰è£ä¾è³´

```yaml
dependencies:
  supabase_flutter: ^2.0.0
  flutter_dotenv: ^5.0.2
```

### 2. åˆå§‹åŒ– Supabase

```dart
// lib/main.dart
import 'package:supabase_flutter/supabase_flutter.dart';
import 'package:flutter_dotenv/flutter_dotenv.dart';

Future<void> main() async {
  WidgetsFlutterBinding.ensureInitialized();
  
  // è¼‰å…¥ç’°å¢ƒè®Šæ•¸
  await dotenv.load(fileName: ".env");
  
  // åˆå§‹åŒ– Supabase
  await Supabase.initialize(
    url: dotenv.env['SUPABASE_URL']!,
    anonKey: dotenv.env['SUPABASE_ANON_KEY']!,
  );
  
  runApp(MyApp());
}

// å…¨åŸŸ Supabase Client
final supabase = Supabase.instance.client;
```

### 3. é‡æ§‹ Service å±¤

ç¯„ä¾‹ï¼š`WorkoutService`

```dart
// lib/services/workout_service.dart
import 'package:supabase_flutter/supabase_flutter.dart';

class WorkoutService {
  final SupabaseClient _client = Supabase.instance.client;
  
  /// å–å¾—ç”¨æˆ¶çš„è¨“ç·´è¨ˆåŠƒ
  Future<List<WorkoutPlan>> getUserWorkoutPlans(String userId) async {
    final response = await _client
        .from('workout_plans')
        .select('''
          *,
          workout_exercises (
            *,
            workout_sets (*)
          )
        ''')
        .eq('trainee_id', userId)
        .order('scheduled_date', ascending: false);
    
    return (response as List)
        .map((json) => WorkoutPlan.fromJson(json))
        .toList();
  }
  
  /// å‰µå»ºè¨“ç·´è¨ˆåŠƒ
  Future<void> createWorkoutPlan(WorkoutPlan plan) async {
    await _client.from('workout_plans').insert(plan.toJson());
  }
  
  /// æ›´æ–°çµ„æ•¸è¨˜éŒ„
  Future<void> updateSet(String setId, {int? reps, double? weight}) async {
    await _client
        .from('workout_sets')
        .update({
          if (reps != null) 'reps': reps,
          if (weight != null) 'weight': weight,
          'completed': true,
        })
        .eq('id', setId);
  }
}
```

---

## âš¡ æ•ˆèƒ½å„ªåŒ–

### 1. ç´¢å¼•ç­–ç•¥

å·²åœ¨ Schema ä¸­å®šç¾©ï¼Œç¢ºä¿å¸¸ç”¨æŸ¥è©¢é«˜æ•ˆï¼š
- `workout_plans` çš„ `trainee_id` + `scheduled_date`
- `exercises` çš„ `training_type` + `body_part`

### 2. æŸ¥è©¢å„ªåŒ–

ä½¿ç”¨ `.select()` çš„ nested query æ¸›å°‘å¾€è¿”æ¬¡æ•¸ï¼š

```dart
// âœ… å¥½ï¼šä¸€æ¬¡æŸ¥è©¢å–å¾—å®Œæ•´è¨ˆåŠƒ
.select('*, workout_exercises(*, workout_sets(*))')

// âŒ å£ï¼šå¤šæ¬¡æŸ¥è©¢
final plan = await getPlan();
for (exercise in plan.exercises) {
  final sets = await getSets(exercise.id); // N+1 å•é¡Œ
}
```

### 3. é›¢ç·šå„ªå…ˆï¼ˆæœªä¾†ï¼‰

å¯é¸æ•´åˆ PowerSync å¯¦ç¾æœ¬åœ° SQLite åŒæ­¥ã€‚

---

## âœ… é©—è­‰æ¸…å–®

- [ ] Schema å‰µå»ºæˆåŠŸ
- [ ] RLS ç­–ç•¥æ¸¬è©¦é€šé
- [ ] è³‡æ–™é·ç§»è…³æœ¬åŸ·è¡ŒæˆåŠŸ
- [ ] è³‡æ–™å®Œæ•´æ€§é©—è­‰ï¼ˆç­†æ•¸ã€æ¬„ä½ï¼‰
- [ ] Flutter æ•´åˆæ¸¬è©¦
- [ ] æ•ˆèƒ½æ¸¬è©¦ï¼ˆæŸ¥è©¢é€Ÿåº¦ï¼‰
- [ ] å‚™ä»½åŸå§‹ Firestore è³‡æ–™

---

## ğŸ“š åƒè€ƒè³‡æº

- [Supabase å®˜æ–¹æ–‡æª”](https://supabase.com/docs)
- [Supabase Flutter SDK](https://supabase.com/docs/reference/dart/introduction)
- [å¾ Firebase é·ç§»åˆ° Supabase](https://supabase.com/docs/guides/migrations/firebase)
- [PostgreSQL ç´¢å¼•å„ªåŒ–](https://www.postgresql.org/docs/current/indexes.html)

---

**æ›´æ–°æ—¥æœŸ**: 2025-12-25  
**ç‹€æ…‹**: ğŸš€ åŸ·è¡Œä¸­

