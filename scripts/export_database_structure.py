#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
StrengthWise è³‡æ–™åº«å®Œæ•´åŒ¯å‡ºå·¥å…·
ç”¨æ–¼è©•ä¼°è³‡æ–™åº«é·ç§»å¯è¡Œæ€§

æ­¤è…³æœ¬æœƒï¼š
1. é€£æ¥ Firebase Firestore
2. åŒ¯å‡ºæ‰€æœ‰é›†åˆçš„å®Œæ•´çµæ§‹
3. åˆ†ææ¬„ä½ä½¿ç”¨æƒ…æ³
4. è©•ä¼°æŸ¥è©¢æ¨¡å¼å’Œæ½›åœ¨æˆæœ¬
5. ç”¢ç”Ÿé©åˆçµ¦è³‡æ–™åº«å°ˆå®¶çœ‹çš„è©³ç´°å ±å‘Š
"""

import sys
import os

# è¨­ç½®è¼¸å‡ºç·¨ç¢¼ç‚º UTF-8
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import firebase_admin
from firebase_admin import credentials, firestore
import json
from collections import defaultdict
from typing import Dict, List, Set, Any, Optional
from datetime import datetime
from pathlib import Path

def initialize_firebase():
    """åˆå§‹åŒ– Firebase Admin SDK"""
    
    # å°‹æ‰¾æœå‹™å¸³è™Ÿé‡‘é‘°æª”æ¡ˆ
    key_file = 'strengthwise-service-account.json'
    key_path = Path(key_file)
    
    if not key_path.exists():
        print(f"[éŒ¯èª¤] æ‰¾ä¸åˆ°æœå‹™å¸³è™Ÿé‡‘é‘°æª”æ¡ˆ: {key_file}")
        print("è«‹ç¢ºä¿æª”æ¡ˆå­˜åœ¨æ–¼å°ˆæ¡ˆæ ¹ç›®éŒ„")
        return False
    
    try:
        cred = credentials.Certificate(str(key_path))
        firebase_admin.initialize_app(cred, {'projectId': 'strengthwise-91f02'})
        print(f"âœ… Firebase åˆå§‹åŒ–æˆåŠŸ")
        return True
    except Exception as e:
        print(f"[éŒ¯èª¤] Firebase åˆå§‹åŒ–å¤±æ•—: {e}")
        return False

def get_field_type(value: Any) -> str:
    """ç²å–æ¬„ä½é¡å‹ï¼ˆç¹é«”ä¸­æ–‡æè¿°ï¼‰"""
    if value is None:
        return "ç©ºå€¼ (null)"
    elif isinstance(value, bool):
        return "å¸ƒæ—å€¼ (boolean)"
    elif isinstance(value, int):
        return "æ•´æ•¸ (integer)"
    elif isinstance(value, float):
        return "æµ®é»æ•¸ (float)"
    elif isinstance(value, str):
        return "å­—ä¸² (string)"
    elif isinstance(value, list):
        if len(value) > 0:
            inner_type = get_field_type(value[0])
            return f"é™£åˆ— (array<{inner_type}>)"
        return "é™£åˆ— (array)"
    elif isinstance(value, dict):
        return "ç‰©ä»¶ (map/object)"
    elif hasattr(value, 'seconds'):  # Timestamp
        return "æ™‚é–“æˆ³è¨˜ (timestamp)"
    elif hasattr(value, 'latitude'):  # GeoPoint
        return "åœ°ç†ä½ç½® (geopoint)"
    else:
        return f"å…¶ä»–é¡å‹ ({type(value).__name__})"

def get_nested_fields(data: Dict[str, Any], prefix: str = "") -> Dict[str, Any]:
    """éè¿´è§£æå·¢ç‹€æ¬„ä½çµæ§‹"""
    fields = {}
    
    for key, value in data.items():
        field_path = f"{prefix}.{key}" if prefix else key
        field_type = get_field_type(value)
        
        fields[field_path] = {
            'type': field_type,
            'value': value
        }
        
        # å¦‚æœæ˜¯ç‰©ä»¶ï¼Œéè¿´è§£æ
        if isinstance(value, dict):
            nested = get_nested_fields(value, field_path)
            fields.update(nested)
        
        # å¦‚æœæ˜¯é™£åˆ—ä¸”åŒ…å«ç‰©ä»¶ï¼Œè§£æç¬¬ä¸€å€‹ç‰©ä»¶çš„çµæ§‹
        elif isinstance(value, list) and len(value) > 0 and isinstance(value[0], dict):
            nested = get_nested_fields(value[0], f"{field_path}[0]")
            fields.update(nested)
    
    return fields

def analyze_collection_deep(collection_name: str, max_docs: int = 1000) -> Dict[str, Any]:
    """æ·±åº¦åˆ†æé›†åˆçµæ§‹"""
    print(f"\nğŸ“Š æ­£åœ¨åˆ†æé›†åˆ: {collection_name}")
    
    db = firestore.client()
    collection_ref = db.collection(collection_name)
    
    try:
        # ç²å–æ–‡æª”
        docs = list(collection_ref.limit(max_docs).stream())
        total_docs = len(docs)
        
        print(f"   æ‰¾åˆ° {total_docs} å€‹æ–‡æª”")
        
        if total_docs == 0:
            return {
                'name': collection_name,
                'document_count': 0,
                'fields': {},
                'samples': []
            }
        
        # åˆ†ææ¬„ä½
        field_stats = defaultdict(lambda: {
            'types': set(),
            'null_count': 0,
            'non_null_count': 0,
            'sample_values': [],
            'nested_fields': set()
        })
        
        sample_docs = []
        doc_ids = []
        
        for doc in docs:
            doc_data = doc.to_dict()
            doc_ids.append(doc.id)
            
            # ä¿å­˜å‰ 5 å€‹å®Œæ•´æ–‡æª”ä½œç‚ºç¯„ä¾‹
            if len(sample_docs) < 5:
                sample_docs.append({
                    'id': doc.id,
                    'data': doc_data
                })
            
            # ç²å–æ‰€æœ‰æ¬„ä½ï¼ˆåŒ…å«å·¢ç‹€ï¼‰
            all_fields = get_nested_fields(doc_data)
            
            for field_path, field_info in all_fields.items():
                field_value = field_info['value']
                field_type = field_info['type']
                
                field_stats[field_path]['types'].add(field_type)
                
                if field_value is None:
                    field_stats[field_path]['null_count'] += 1
                else:
                    field_stats[field_path]['non_null_count'] += 1
                    
                    # ä¿å­˜ç¯„ä¾‹å€¼ï¼ˆæœ€å¤š 5 å€‹ï¼‰
                    if len(field_stats[field_path]['sample_values']) < 5:
                        if isinstance(field_value, (str, int, float, bool)):
                            field_stats[field_path]['sample_values'].append(field_value)
                        elif isinstance(field_value, list):
                            field_stats[field_path]['sample_values'].append(f"[é™£åˆ—, {len(field_value)} é …]")
                        elif isinstance(field_value, dict):
                            field_stats[field_path]['sample_values'].append(f"{{ç‰©ä»¶, {len(field_value)} æ¬„ä½}}")
                        elif hasattr(field_value, 'seconds'):
                            # Timestamp
                            dt = datetime.fromtimestamp(field_value.seconds)
                            field_stats[field_path]['sample_values'].append(dt.strftime('%Y-%m-%d %H:%M:%S'))
        
        # æ ¼å¼åŒ–æ¬„ä½çµ±è¨ˆ
        fields_info = {}
        for field_path, stats in field_stats.items():
            occurrence_count = stats['null_count'] + stats['non_null_count']
            occurrence_rate = round(occurrence_count / total_docs * 100, 2)
            null_rate = round(stats['null_count'] / occurrence_count * 100, 2) if occurrence_count > 0 else 0
            
            fields_info[field_path] = {
                'types': sorted(list(stats['types'])),
                'occurrence_count': occurrence_count,
                'occurrence_rate': occurrence_rate,
                'null_count': stats['null_count'],
                'non_null_count': stats['non_null_count'],
                'null_rate': null_rate,
                'sample_values': stats['sample_values'][:5]
            }
        
        # çµ±è¨ˆè³‡è¨Š
        total_fields = len(fields_info)
        avg_fields_per_doc = round(total_fields / total_docs, 2) if total_docs > 0 else 0
        
        print(f"   âœ… åˆ†æå®Œæˆ: {total_fields} å€‹æ¬„ä½ï¼ˆå«å·¢ç‹€ï¼‰")
        
        return {
            'name': collection_name,
            'document_count': total_docs,
            'total_fields': total_fields,
            'avg_fields_per_doc': avg_fields_per_doc,
            'fields': fields_info,
            'sample_documents': sample_docs,
            'sample_doc_ids': doc_ids[:10]
        }
        
    except Exception as e:
        print(f"   âŒ éŒ¯èª¤: {e}")
        return {
            'name': collection_name,
            'error': str(e)
        }

def estimate_query_costs(collections: Dict[str, Any]) -> Dict[str, Any]:
    """ä¼°ç®—å¸¸è¦‹æŸ¥è©¢æ¨¡å¼çš„æˆæœ¬"""
    
    print("\nğŸ’° æ­£åœ¨ä¼°ç®—æŸ¥è©¢æˆæœ¬...")
    
    costs = {
        'description': 'åŸºæ–¼ Firestore å®šåƒ¹ï¼šè®€å– $0.06/100K æ¬¡ï¼Œå¯«å…¥ $0.18/100K æ¬¡',
        'scenarios': []
    }
    
    # åˆ†æ workoutPlans çš„æŸ¥è©¢æ¨¡å¼
    if 'workoutPlans' in collections:
        workout_count = collections['workoutPlans'].get('document_count', 0)
        
        costs['scenarios'].append({
            'name': 'ç”¨æˆ¶è¼‰å…¥è¨“ç·´è¨ˆåŠƒåˆ—è¡¨',
            'description': 'æ¯æ¬¡æ‰“é–‹ App æŸ¥è©¢ traineeId',
            'estimated_reads_per_query': min(50, workout_count),
            'frequency': 'æ¯ç”¨æˆ¶æ¯æ—¥ 5-10 æ¬¡',
            'monthly_cost_per_user': round((50 * 8 * 30) / 100000 * 0.06, 4),
            'note': 'è‹¥ç”¨æˆ¶æœ‰å¤§é‡æ­·å²è¨˜éŒ„ï¼Œæˆæœ¬æœƒç·šæ€§å¢åŠ '
        })
        
        costs['scenarios'].append({
            'name': 'å®Œæˆä¸€æ¬¡è¨“ç·´',
            'description': 'è®€å–æ¨¡æ¿ + æ›´æ–°è¨˜éŒ„',
            'estimated_reads': 1,
            'estimated_writes': 1,
            'frequency': 'æ¯ç”¨æˆ¶æ¯é€± 3-5 æ¬¡',
            'monthly_cost_per_user': round((1 * 4 * 4) / 100000 * 0.06 + (1 * 4 * 4) / 100000 * 0.18, 4),
        })
    
    # åˆ†æ exercises çš„æŸ¥è©¢æ¨¡å¼
    if 'exercises' in collections or 'exercise' in collections:
        exercise_coll = 'exercises' if 'exercises' in collections else 'exercise'
        exercise_count = collections[exercise_coll].get('document_count', 0)
        
        costs['scenarios'].append({
            'name': 'è¼‰å…¥å‹•ä½œè³‡æ–™åº«',
            'description': 'ç”¨æˆ¶é¸æ“‡å‹•ä½œæ™‚æŸ¥è©¢æ‰€æœ‰å‹•ä½œ',
            'estimated_reads_per_query': exercise_count,
            'frequency': 'æ¯ç”¨æˆ¶æ¯é€± 1-2 æ¬¡',
            'monthly_cost_per_user': round((exercise_count * 2 * 4) / 100000 * 0.06, 4),
            'note': f'å…± {exercise_count} å€‹å‹•ä½œï¼Œæ¯æ¬¡éƒ½éœ€è¦è®€å–å…¨éƒ¨'
        })
    
    # åˆ†æ users çš„æŸ¥è©¢æ¨¡å¼
    if 'users' in collections:
        costs['scenarios'].append({
            'name': 'ç”¨æˆ¶ç™»å…¥',
            'description': 'æŸ¥è©¢ç”¨æˆ¶è³‡æ–™',
            'estimated_reads': 1,
            'frequency': 'æ¯ç”¨æˆ¶æ¯æ—¥ 1-3 æ¬¡',
            'monthly_cost_per_user': round((1 * 2 * 30) / 100000 * 0.06, 4),
        })
    
    return costs

def generate_migration_recommendations(collections: Dict[str, Any]) -> List[str]:
    """ç”¢ç”Ÿè³‡æ–™åº«é·ç§»å»ºè­°"""
    
    recommendations = []
    
    # æª¢æŸ¥ workoutPlans çš„è¦æ¨¡
    if 'workoutPlans' in collections:
        workout_count = collections['workoutPlans'].get('document_count', 0)
        if workout_count > 1000:
            recommendations.append({
                'priority': 'é«˜',
                'issue': f'workoutPlans é›†åˆå·²æœ‰ {workout_count} å€‹æ–‡æª”',
                'impact': 'æ¯æ¬¡æŸ¥è©¢ç”¨æˆ¶è¨“ç·´è¨ˆåŠƒéƒ½éœ€è¦æƒæå¤§é‡æ–‡æª”ï¼Œæˆæœ¬éš¨æ™‚é–“ç·šæ€§å¢é•·',
                'suggestion': 'è€ƒæ…®ä½¿ç”¨é—œè¯å¼è³‡æ–™åº«ï¼ˆPostgreSQLï¼‰é…åˆç´¢å¼•ï¼Œæˆ–åˆ†ç‰‡å­˜å„²æ­·å²è¨˜éŒ„'
            })
    
    # æª¢æŸ¥ exercises çš„è¦æ¨¡
    exercise_coll = None
    if 'exercises' in collections:
        exercise_coll = 'exercises'
    elif 'exercise' in collections:
        exercise_coll = 'exercise'
    
    if exercise_coll:
        exercise_count = collections[exercise_coll].get('document_count', 0)
        if exercise_count > 500:
            recommendations.append({
                'priority': 'ä¸­',
                'issue': f'{exercise_coll} é›†åˆæœ‰ {exercise_count} å€‹å‹•ä½œ',
                'impact': 'å‹•ä½œè³‡æ–™å¹¾ä¹ä¸è®Šï¼Œä½†æ¯æ¬¡éƒ½è¦å¾ Firestore è®€å–',
                'suggestion': 'å‹•ä½œè³‡æ–™å¯ä»¥ï¼š1) æ‰“åŒ…é€² App å…§ï¼Œ2) ä½¿ç”¨ CDN å¿«å–ï¼Œ3) é·ç§»åˆ° PostgreSQL ä¸¦é…åˆ Redis å¿«å–'
            })
    
    # æª¢æŸ¥æŸ¥è©¢æ¨¡å¼
    recommendations.append({
        'priority': 'é«˜',
        'issue': 'Firestore ä¸æ”¯æ´è¤‡é›œæŸ¥è©¢',
        'impact': 'éœ€è¦å®¢æˆ¶ç«¯æ’åº/éæ¿¾ï¼Œæˆ–å‰µå»ºå¤§é‡è¤‡åˆç´¢å¼•',
        'suggestion': 'é—œè¯å¼è³‡æ–™åº«ï¼ˆPostgreSQLï¼‰å°è¤‡é›œæŸ¥è©¢æœ‰åŸç”Ÿæ”¯æ´ï¼Œä¸”æˆæœ¬æ›´å¯é æ¸¬'
    })
    
    # æˆæœ¬é æ¸¬
    recommendations.append({
        'priority': 'é«˜',
        'issue': 'Firestore æˆæœ¬éš¨ç”¨æˆ¶å¢é•·ä¸å¯é æ¸¬',
        'impact': '1000 æ´»èºç”¨æˆ¶å¯èƒ½ç”¢ç”Ÿæ¯æœˆ $50-200 çš„è®€å–æˆæœ¬',
        'suggestion': 'é—œè¯å¼è³‡æ–™åº«ï¼ˆå¦‚ Supabase PostgreSQLï¼‰æä¾›å›ºå®šæœˆè²»ï¼Œæ›´é©åˆè¦æ¨¡åŒ–'
    })
    
    return recommendations

def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 80)
    print("StrengthWise è³‡æ–™åº«å®Œæ•´åŒ¯å‡ºå·¥å…·")
    print("=" * 80)
    print("ç”¨é€”: è©•ä¼°è³‡æ–™åº«é·ç§»å¯è¡Œæ€§\n")
    
    # åˆå§‹åŒ– Firebase
    if not initialize_firebase():
        sys.exit(1)
    
    # å®šç¾©è¦åˆ†æçš„é›†åˆ
    known_collections = [
        'users',
        'user',
        'workoutPlans',
        'bookings',
        'exercise',
        'exercises',
        'bodyParts',
        'exerciseTypes',
        'notes',
        'relationships',
        'availabilities',
    ]
    
    print(f"\nğŸ” é–‹å§‹æƒæ {len(known_collections)} å€‹å·²çŸ¥é›†åˆ...")
    
    # åˆ†ææ‰€æœ‰é›†åˆ
    db = firestore.client()
    collections_data = {}
    
    for collection_name in known_collections:
        try:
            # å…ˆæª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            test_query = db.collection(collection_name).limit(1).stream()
            if list(test_query):
                result = analyze_collection_deep(collection_name, max_docs=1000)
                if 'error' not in result:
                    collections_data[collection_name] = result
        except Exception as e:
            print(f"   âš ï¸  é›†åˆ {collection_name} ä¸å­˜åœ¨æˆ–ç„¡æ³•å­˜å–")
            continue
    
    print(f"\nâœ… æˆåŠŸåˆ†æ {len(collections_data)} å€‹é›†åˆ")
    
    # ä¼°ç®—æŸ¥è©¢æˆæœ¬
    query_costs = estimate_query_costs(collections_data)
    
    # ç”¢ç”Ÿé·ç§»å»ºè­°
    recommendations = generate_migration_recommendations(collections_data)
    
    # çµ„è£å®Œæ•´å ±å‘Š
    report = {
        'project_id': 'strengthwise-91f02',
        'export_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'collections': collections_data,
        'query_cost_analysis': query_costs,
        'migration_recommendations': recommendations,
        'summary': {
            'total_collections': len(collections_data),
            'total_documents': sum(c.get('document_count', 0) for c in collections_data.values()),
            'total_fields': sum(c.get('total_fields', 0) for c in collections_data.values()),
        }
    }
    
    # å„²å­˜ JSON å ±å‘Š
    output_file = 'database_export_for_migration.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ“„ JSON å ±å‘Šå·²å„²å­˜: {output_file}")
    
    # ç”¢ç”Ÿ Markdown å ±å‘Š
    generate_markdown_report(report)
    
    # åˆ—å°æ‘˜è¦
    print("\n" + "=" * 80)
    print("ğŸ“Š è³‡æ–™åº«æ‘˜è¦")
    print("=" * 80)
    print(f"é›†åˆæ•¸é‡: {report['summary']['total_collections']}")
    print(f"æ–‡æª”ç¸½æ•¸: {report['summary']['total_documents']}")
    print(f"æ¬„ä½ç¸½æ•¸: {report['summary']['total_fields']}")
    print("\nğŸ’¡ è©³ç´°å ±å‘Šè«‹æŸ¥çœ‹: docs/database_migration_analysis.md")
    print("=" * 80)

def generate_markdown_report(report: Dict[str, Any]):
    """ç”¢ç”Ÿ Markdown æ ¼å¼çš„å°ˆæ¥­å ±å‘Š"""
    
    output_file = 'docs/database_migration_analysis.md'
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# StrengthWise è³‡æ–™åº«é·ç§»è©•ä¼°å ±å‘Š\n\n")
        f.write(f"**å°ˆæ¡ˆ**: {report['project_id']}\n")
        f.write(f"**åŒ¯å‡ºæ™‚é–“**: {report['export_time']}\n")
        f.write(f"**ç›®çš„**: è©•ä¼°å¾ Firebase Firestore é·ç§»åˆ°å…¶ä»–è³‡æ–™åº«çš„å¯è¡Œæ€§\n\n")
        
        f.write("---\n\n")
        
        # åŸ·è¡Œæ‘˜è¦
        f.write("## ğŸ“Š åŸ·è¡Œæ‘˜è¦\n\n")
        f.write(f"- **é›†åˆæ•¸é‡**: {report['summary']['total_collections']} å€‹\n")
        f.write(f"- **æ–‡æª”ç¸½æ•¸**: {report['summary']['total_documents']} å€‹\n")
        f.write(f"- **æ¬„ä½ç¸½æ•¸**: {report['summary']['total_fields']} å€‹ï¼ˆå«å·¢ç‹€æ¬„ä½ï¼‰\n\n")
        
        # é·ç§»å»ºè­°
        f.write("## ğŸ¯ é·ç§»å»ºè­°\n\n")
        
        for idx, rec in enumerate(report['migration_recommendations'], 1):
            f.write(f"### {idx}. {rec['issue']} `[å„ªå…ˆç´š: {rec['priority']}]`\n\n")
            f.write(f"**å½±éŸ¿**: {rec['impact']}\n\n")
            f.write(f"**å»ºè­°**: {rec['suggestion']}\n\n")
        
        # æŸ¥è©¢æˆæœ¬åˆ†æ
        f.write("## ğŸ’° æŸ¥è©¢æˆæœ¬åˆ†æ\n\n")
        f.write(f"> {report['query_cost_analysis']['description']}\n\n")
        
        f.write("### å¸¸è¦‹æŸ¥è©¢å ´æ™¯\n\n")
        
        for scenario in report['query_cost_analysis']['scenarios']:
            f.write(f"#### {scenario['name']}\n\n")
            f.write(f"- **èªªæ˜**: {scenario['description']}\n")
            f.write(f"- **é »ç‡**: {scenario['frequency']}\n")
            
            if 'estimated_reads_per_query' in scenario:
                f.write(f"- **æ¯æ¬¡è®€å–æ•¸**: {scenario['estimated_reads_per_query']}\n")
            if 'estimated_reads' in scenario:
                f.write(f"- **è®€å–æ¬¡æ•¸**: {scenario['estimated_reads']}\n")
            if 'estimated_writes' in scenario:
                f.write(f"- **å¯«å…¥æ¬¡æ•¸**: {scenario['estimated_writes']}\n")
            if 'monthly_cost_per_user' in scenario:
                f.write(f"- **æ¯ç”¨æˆ¶æœˆæˆæœ¬**: ${scenario['monthly_cost_per_user']}\n")
            if 'note' in scenario:
                f.write(f"- **å‚™è¨»**: {scenario['note']}\n")
            
            f.write("\n")
        
        # é›†åˆè©³ç´°çµæ§‹
        f.write("## ğŸ“ é›†åˆè©³ç´°çµæ§‹\n\n")
        
        for collection_name, collection_data in sorted(report['collections'].items()):
            if 'error' in collection_data:
                continue
            
            f.write(f"### {collection_name}\n\n")
            f.write(f"- **æ–‡æª”æ•¸é‡**: {collection_data['document_count']}\n")
            f.write(f"- **æ¬„ä½æ•¸é‡**: {collection_data['total_fields']}ï¼ˆå«å·¢ç‹€ï¼‰\n")
            f.write(f"- **å¹³å‡æ¬„ä½æ•¸/æ–‡æª”**: {collection_data['avg_fields_per_doc']}\n\n")
            
            # æ¬„ä½è¡¨æ ¼
            f.write("#### æ¬„ä½æ¸…å–®\n\n")
            f.write("| æ¬„ä½è·¯å¾‘ | é¡å‹ | å‡ºç¾ç‡ | ç©ºå€¼ç‡ | ç¯„ä¾‹å€¼ |\n")
            f.write("|---------|------|--------|--------|--------|\n")
            
            for field_path, field_info in sorted(collection_data['fields'].items()):
                types_str = ", ".join(field_info['types'])
                occurrence_rate = f"{field_info['occurrence_rate']}%"
                null_rate = f"{field_info['null_rate']}%"
                
                # ç¯„ä¾‹å€¼
                if field_info['sample_values']:
                    sample = str(field_info['sample_values'][0])
                    if len(sample) > 40:
                        sample = sample[:37] + "..."
                else:
                    sample = "-"
                
                # è™•ç† Markdown è¡¨æ ¼ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                field_path_escaped = field_path.replace('|', '\\|')
                sample_escaped = sample.replace('|', '\\|')
                
                f.write(f"| `{field_path_escaped}` | {types_str} | {occurrence_rate} | {null_rate} | {sample_escaped} |\n")
            
            f.write("\n")
            
            # ç¯„ä¾‹æ–‡æª”
            if collection_data.get('sample_documents'):
                f.write("#### ç¯„ä¾‹æ–‡æª”\n\n")
                f.write("```json\n")
                f.write(json.dumps(collection_data['sample_documents'][0], ensure_ascii=False, indent=2, default=str))
                f.write("\n```\n\n")
        
        # é™„éŒ„
        f.write("---\n\n")
        f.write("## ğŸ“ é™„éŒ„\n\n")
        f.write("### æ¨è–¦çš„æ›¿ä»£æ–¹æ¡ˆ\n\n")
        f.write("1. **Supabase (PostgreSQL)**\n")
        f.write("   - å®Œæ•´çš„ SQL åŠŸèƒ½\n")
        f.write("   - å›ºå®šæœˆè²»ï¼ˆ$25 èµ·ï¼‰\n")
        f.write("   - å…§å»ºå³æ™‚è¨‚é–±\n")
        f.write("   - å®Œæ•´çš„ Flutter SDK\n\n")
        
        f.write("2. **è‡ªæ¶ PostgreSQL + Redis**\n")
        f.write("   - å®Œå…¨å¯æ§\n")
        f.write("   - æˆæœ¬æœ€ä½ï¼ˆé•·æœŸï¼‰\n")
        f.write("   - éœ€è¦ç¶­è­·\n\n")
        
        f.write("3. **ä¿ç•™ Firestore ä½†å„ªåŒ–**\n")
        f.write("   - åˆ†é›¢éœæ…‹è³‡æ–™ï¼ˆexercisesï¼‰åˆ° CDN\n")
        f.write("   - å¯¦ä½œæ›´å¤šå®¢æˆ¶ç«¯å¿«å–\n")
        f.write("   - å®šæœŸå°å­˜æ­·å²è³‡æ–™\n\n")
    
    print(f"ğŸ“„ Markdown å ±å‘Šå·²å„²å­˜: {output_file}")

if __name__ == '__main__':
    main()

